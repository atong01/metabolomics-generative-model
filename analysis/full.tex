\documentclass[11pt]{article}

% -- Semantics stuff
%     I defined a couple of special commands (see examples in the text below) to make
%     writing inference rules and judgements easier.
\newcommand{\br}[1]{\langle #1 \rangle}
\def\Yields{\Downarrow}

% -- Page size
\textheight     9.0truein
\textwidth      6.5truein
\topmargin     -0.5truein
\oddsidemargin  +0.0truein
\evensidemargin +0.0truein
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{fullpage}
\usepackage{enumerate}
\usepackage[mathscr]{euscript}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\renewcommand{\theenumi}{\roman{enumi}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}


% -- Document title (appears at top)
\title{Machine Learning in Untargeted Metabolomics: Final report for c150}
\author{Alex Tong}
\begin{document}
\maketitle

\section{Background}

I've been working on this project starting in February 2016 with Soha Hassoun. My contribution has been in the probabilistic model design and test. In the spring we started with a Bayesian Network designed in BayesNetToolkit. \cite{bayesnet}

\section{Introduction}

\section{Model Specification}
Parameters:
\begin{itemize}
\item[$\lambda_p$:] Probability that a pathway is active
\item[$\mu_0$:] Probability that a feature is present given inactive pathway
\item[$\mu_1$:] Probability that a feature is present given active pathway
\end{itemize}

Variables:
\begin{itemize}
\item [$a_p$:] IRV indicating pathway $p$ activity
\item [$b_{p,f}$:] IRV indicating feature f is associated with pathway p
\item[$o_{p,f}$:] IRV indicating whether feature $f$ associated with pathway $p$ is present in the sample due to pathway $p$
\item[$m_f$:] IRV indicating whether feature $f$ is present in the sample
\item[$v_f$:] IRV (virtual evidence on feature f
\end{itemize}

Generative Model Prior:
\begin{itemize}
\item [$P_p$:] $Bernoulli(\lambda_p)$ for $p = 1 ... P$
\item [$o_{p,f} | P_p, \mu$:] $Bernoulli(\mu_{P_p})$ for $f$ in Features($p$)
\item [$M_f$]  $= (1 -  \prod_p (1 - o_{p,f}))$ Equivalent to logical OR
\item [$v_f$] $= Bernoulli$(Measured P($f$))
\end{itemize}

Observation:
\begin{itemize}
\item [$v_f$] $ = 1$
\end{itemize}

Posterior:

\begin{align}
p(o | \lambda, \mu_0, \mu_1, b_{p,f}, a_p) &= \prod_p \prod_{f} (\mu_{a_p}^{o_{p,f}} (1-\mu_{a_p})^{(1-o_{p,f})})^{b_{p,f}} \\
p(m | o, b) &= \prod_f m_f = \prod_f (1 - \prod_p (1 - o_{p,f})^{b_{p,f}}) \\
p(\lambda, \mu_0, \mu_1, a, o, m) &= p(a | \lambda )p(\lambda) p(\mu_0) p(\mu_1) p( o | \lambda, \mu_0, \mu_1, b_{p,f}) p(m | o) \\
p(\lambda, \mu_0, \mu_1, a, o, m | v = \mathbf{1}) &= \frac{p(v | \lambda, \mu_0, \mu_1, a, o, y) * p(\lambda, \mu_0, \mu_1, a, o, m)}{p(v = \mathbf{1})}  \\
&\propto p(v | m) * p(\lambda, \mu_0, \mu_1, a, o, m) 
\end{align}
%p(\alpha | v = \mathbf{1}) &= 

Description:
Equation 1 shows the likelihood of a given set of $o$ variables. For example, if I wanted to calculate the probability of all $o_{p,f}$ variables being zero, I would need all given hyperparameters, $\lambda, \mu_0, \mu_1$, and the values of $a_p$. The likelihood as stated is a function of $p$ variables, $a_{1...p}$. Note that with this likelihood function, it is simple to calculate the likelihood $P(o | \lambda, \mu_0, \mu_1, a)$, In fact, for a given $o_{p,f}$, we can calculate $p(o_{p,f} | a_p, b_{p,f}) = \mu_{a_p}^{o_{p,f}} (1-\mu_{a_p})^{(1-o_{p,f})}$ or 
$$p(o_{p,f} = 1 | a_p, b_{p,f}) = \mu_{a_p}$$
$$p(o_{p,f} = 0 | a_p, b_{p,f}) = 1-\mu_{a_p}$$
Equation 2 shows the likelihood of a set of metabolite observations given $o$. for example, the probability of getting $m_1 = 1, m_2 = 0, m_3 = 1$ given all of $o$, is a constant.\\\\
Equation 3 shows the likelihood over all hidden variables. This is derived from looking at our bayesian network, as each variable is independent.\\\\
Equation 4 shows the model likelihood given our observation of our virtual nodes. This is derived from bayes rule. \\\\

$$p(v_f | m_f) = P(metfrag)  * P(\pi)$$

Reasonable Values:
\begin{itemize}
\item $\pi$ should be nominally quite low, and may be lower for some metabolites than others possibly with the idea that larger molecules are harder to detect as there are more possible fragments.
\item We will start with $\mu_0 = 0.001$ and $\mu_1 = 0.999$ as values very close to 0 and 1. 
\item We will start with $\lambda = 0.5$, but would like to move to a model where we can incorporate more reasonable priors separately on each pathway, something like a $\lambda_p$ for $p = 1...P$. 

\end{itemize}

\section{Summary}



\section{Acknowledgments}

Thanks to Nicholas Alden, Vlad Porokhin, Neda Hassanpour, Kyongbum Lee, Soha Hassoun who are also working on this project.

\section{References}

\bibliographystyle{unsrt}
\bibliography{foo}


\end{document}
